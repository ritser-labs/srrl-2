#!/usr/bin/env python3
"""
NUMERICALLY STABLE GRPO vs SRRL Experiment
"""

import os
import json
import torch
import re
import math
from pathlib import Path
from datetime import datetime
import functools, builtins  # Added for auto-flushing prints
print = functools.partial(builtins.print, flush=True)  # Auto-flush all prints
MAX_TRAIN_TOKENS = 1024  # Limit sequence length during training to avoid OOM
from loss import GRPOLoss, approx_kl_divergence

def clear_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def cleanup_model(model):
    """Properly cleanup model to prevent CUDA corruption."""
    if model is not None:
        model.cpu()
        del model
        clear_memory()

def create_code_prompt(problem):
    """Create a clear prompt that instructs LLM to generate Python functions."""
    first_test = problem["test_list"][0] if problem["test_list"] else ""
    function_name = "unknown_function"
    
    match = re.search(r'(\w+)\s*\(', first_test)
    if match:
        function_name = match.group(1)
    
    prompt = f"""Problem: {problem["prompt"]}

Test: {first_test}

Write a Python function named {function_name}. Use this EXACT format:

<reasoning>
Your approach
</reasoning>

<code>
def {function_name}(parameters):
    # working code
    return result
</code>

Start with <reasoning> - no other text first!"""
    
    return prompt

def extract_code_from_completion(completion):
    """Extract Python code from XML-formatted completion."""
    code_match = re.search(r'<code>\s*(.*?)\s*</code>', completion, re.DOTALL)
    if code_match:
        return code_match.group(1).strip()
    
    if "```python" in completion:
        start = completion.find("```python") + 9
        end = completion.find("```", start)
        if end != -1:
            return completion[start:end].strip()
    
    if "```" in completion:
        start = completion.find("```") + 3
        end = completion.find("```", start)
        if end != -1:
            return completion[start:end].strip()
    
    lines = completion.split('\n')
    code_lines = []
    in_function = False
    
    for line in lines:
        if line.strip().startswith('def '):
            in_function = True
            code_lines.append(line)
        elif in_function:
            if line.strip() == "" or line.startswith('    ') or line.startswith('\t'):
                code_lines.append(line)
            else:
                break
    
    if code_lines:
        return '\n'.join(code_lines)
    
    return completion.strip()

def create_refinement_prompt(original_prompt, failed_code, execution_result):
    """Create refinement prompt with execution feedback."""
    function_name = "unknown_function"
    if execution_result.get("failed_tests"):
        first_test = execution_result["failed_tests"][0]
        match = re.search(r'(\w+)\s*\(', first_test)
        if match:
            function_name = match.group(1)
    
    refinement_prompt = f"""The following code attempt failed to solve the problem correctly:

Original Problem:
{original_prompt}

Failed Code:
```python
{failed_code}
```

Execution Result:
"""
    
    if execution_result.get("error"):
        refinement_prompt += f"Error: {execution_result['error']}\n"
    
    if execution_result.get("failed_tests"):
        refinement_prompt += f"Failed Tests:\n"
        for test in execution_result["failed_tests"]:
            refinement_prompt += f"- {test}\n"
    
    refinement_prompt += f"""
Analyze the error and provide a corrected solution. 

Instructions:
1. Fix the issues in the code
2. Ensure the function is named '{function_name}'
3. Make sure it passes all test cases
4. Format your response as:

<reasoning>
Explain what was wrong and how you fixed it
</reasoning>

<code>
def {function_name}(...):
    # Your corrected implementation here
    pass
</code>

Generate the corrected function:"""
    
    return refinement_prompt

def group_advantages(returns: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """Normalize advantages like the original GRPO code."""
    return (returns - returns.mean()) / (returns.std() + eps)

def sequence_log_probs_from_logits(logits: torch.tensor, output_ids: torch.tensor) -> torch.Tensor:
    """Extract log probs like the original code."""
    log_prob = torch.nn.functional.log_softmax(logits, dim=-1)
    return log_prob.gather(dim=-1, index=output_ids.unsqueeze(-1)).squeeze(-1)

def sequences_log_probs(model, sequence_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
    """Get sequence log probs like the original code."""
    position_ids = attention_mask.long().cumsum(dim=-1) - 1
    position_ids.masked_fill_(mask=(attention_mask == 0), value=1)
    output = model.forward(
        input_ids=sequence_ids,
        attention_mask=attention_mask,
        position_ids=position_ids,
        use_cache=False,
    )
    logits = output["logits"]
    log_probs = sequence_log_probs_from_logits(
        logits=logits[:, :-1].to(torch.float32),
        output_ids=sequence_ids[:, 1:],
    )
    return log_probs

# ===============================================
# Helper: sample-wise training to avoid OOM
def stable_train_sample_wise(model, optimizer, tokenizer, sequences, rewards, device, label="GRPO"):
    """Perform a single policy-gradient update sample-by-sample.
    This keeps VRAM usage low and avoids mixing problems together."""
    if not sequences:
        return
    model.train()
    optimizer.zero_grad()
    rewards_tensor = torch.tensor(rewards, device=device, dtype=torch.float32)
    advantages = group_advantages(rewards_tensor).tolist()
    for seq_idx, (seq, adv) in enumerate(zip(sequences, advantages)):
        seq = seq[-MAX_TRAIN_TOKENS:]
        sequence_ids = torch.cat([
            torch.full((MAX_TRAIN_TOKENS - len(seq),), tokenizer.pad_token_id, device=device),
            seq
        ]).unsqueeze(0)
        attention_mask = (sequence_ids != tokenizer.pad_token_id)
        log_probs = sequences_log_probs(model, sequence_ids, attention_mask)
        attn_shifted = attention_mask[:, 1:]
        avg_log_prob = (log_probs * attn_shifted).sum() / attn_shifted.sum().clamp(min=1)
        loss = -(avg_log_prob * adv)
        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        if math.isfinite(grad_norm):
            optimizer.step()
            optimizer.zero_grad()
    print(f"      ‚úÖ {label} sample-wise update done ({len(sequences)} sequences)")
    clear_memory()
# ===============================================

def train_grpo(train_problems, num_steps=1):  # Reduce to 1 step to avoid corruption
    print("\nüöÄ TRAINING GRPO (PROPER LOSS)")
    print("=" * 40)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from mbpp_utils import execute_code_with_tests, calculate_code_reward
        
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # Ensure GPU usage
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
        ).to(device)  # Move model to the chosen device
        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-8, weight_decay=0.01)
        
        print(f"Training on {len(train_problems)} problems")
        
        for step in range(num_steps):
            print(f"Step {step+1}/{num_steps}")
            
            total_rewards = []
            total_sequences = []
            
            for problem in train_problems:
                prompt = create_code_prompt(problem)
                
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=400)
                input_ids = inputs["input_ids"].to(device)
                attention_mask = inputs["attention_mask"].to(device)
                
                model.eval()
                with torch.no_grad():
                    try:
                        outputs = model.generate(
                            input_ids,
                            attention_mask=attention_mask,
                            max_new_tokens=2000,  # Full 2k tokens for complex code
                            do_sample=True,
                            temperature=0.7,
                            top_p=0.9,
                            num_return_sequences=2,
                            pad_token_id=tokenizer.pad_token_id,
                            use_cache=True
                        )
                    except RuntimeError as e:
                        if "CUDA" in str(e):
                            print(f"    ‚ö†Ô∏è CUDA error during generation, skipping problem {problem['task_id']}")
                            continue
                        else:
                            raise
                
                local_rewards = []
                local_sequences = []
                
                for output in outputs:
                    completion = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
                    code = extract_code_from_completion(completion)
                    
                    exec_result = execute_code_with_tests(code, problem["test_list"], problem.get("test_imports", []))
                    reward = calculate_code_reward(exec_result)
                    local_rewards.append(reward)
                    local_sequences.append(output)
                    
                    total_rewards.append(reward)
                    total_sequences.append(output)
                    
                    if reward >= 1.0:
                        print(f"    ‚úÖ Success! Reward: {reward:.2f}")
                    else:
                        print(f"    ‚ùå Failed. Reward: {reward:.2f}")
                
                # --- train per-problem ---
                if any(r > 0 for r in local_rewards):
                    stable_train_sample_wise(model, optimizer, tokenizer, local_sequences, local_rewards, device, label="GRPO")
            # end for problem loop
            
            if not total_rewards:
                print(f"  No valid sequences, skipping training step")
                continue
                
            avg_reward = sum(total_rewards) / len(total_rewards)
            success_rate = sum(1 for r in total_rewards if r >= 1.0) / len(total_rewards)
            print(f"  Avg reward: {avg_reward:.3f}, Success: {success_rate:.1%}")
            
            # Clear model state and memory to prevent corruption
            model.zero_grad(set_to_none=True)
            clear_memory()
        
        output_dir = "./models/grpo"
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print("‚úÖ GRPO completed!")
        
        # Cleanup model properly to prevent CUDA corruption
        cleanup_model(model)
        return True
        
    except Exception as e:
        print(f"‚ùå GRPO failed: {e}")
        import traceback
        traceback.print_exc()
        if 'model' in locals():
            cleanup_model(model)
        return False

def train_srrl(train_problems, num_steps=1):  # Reduce to 1 step to avoid corruption
    print("\nüöÄ TRAINING SRRL (PROPER LOSS)")
    print("=" * 40)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from mbpp_utils import execute_code_with_tests, calculate_code_reward
        
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
        ).to(device)
        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-8, weight_decay=0.01)
        
        print(f"Training on {len(train_problems)} problems")
        
        for step in range(num_steps):
            print(f"Step {step+1}/{num_steps}")
            
            all_rewards = []
            all_sequences = []
            refinement_count = 0
            
            for problem in train_problems:
                prompt = create_code_prompt(problem)
                
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=400)
                input_ids = inputs["input_ids"].to(device)
                attention_mask = inputs["attention_mask"].to(device)
                
                model.eval()
                failed_attempts = []
                
                with torch.no_grad():
                    try:
                        outputs = model.generate(
                            input_ids,
                            attention_mask=attention_mask,
                            max_new_tokens=2000,
                            do_sample=True,
                            temperature=0.7,
                            top_p=0.9,
                            num_return_sequences=2,
                            pad_token_id=tokenizer.pad_token_id,
                            use_cache=True
                        )
                    except RuntimeError as e:
                        if "CUDA" in str(e):
                            print(f"    ‚ö†Ô∏è CUDA error during c1 generation, skipping")
                            continue
                        else:
                            raise
                
                for output in outputs:
                    completion = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)
                    code = extract_code_from_completion(completion)
                    
                    print(f"    Generated c1: {code[:60]}...")
                    
                    exec_result = execute_code_with_tests(code, problem["test_list"], problem.get("test_imports", []))
                    reward = calculate_code_reward(exec_result)
                    
                    all_rewards.append(reward)
                    all_sequences.append(output)
                    
                    if not exec_result["success"]:
                        failed_attempts.append({"code": code, "execution_result": exec_result})
                        print(f"    ‚ùå c1 failed, adding to refinement")
                    else:
                        print(f"    ‚úÖ c1 success! Reward: {reward:.2f}")
                
                if failed_attempts:
                    print(f"  SRRL: Refining {len(failed_attempts)} failures...")
                    
                    for attempt in failed_attempts[:1]:
                        refined_prompt = create_refinement_prompt(problem["prompt"], attempt["code"], attempt["execution_result"])
                        
                        ref_inputs = tokenizer(refined_prompt, return_tensors="pt", truncation=True, max_length=600)
                        ref_input_ids = ref_inputs["input_ids"].to(device)
                        ref_attention_mask = ref_inputs["attention_mask"].to(device)
                        
                        with torch.no_grad():
                            try:
                                ref_outputs = model.generate(
                                    ref_input_ids,
                                    attention_mask=ref_attention_mask,
                                    max_new_tokens=2000,
                                    do_sample=True,
                                    temperature=0.7,
                                    top_p=0.9,
                                    num_return_sequences=1,
                                    pad_token_id=tokenizer.pad_token_id,
                                    use_cache=True
                                )
                            except RuntimeError as e:
                                if "CUDA" in str(e):
                                    print(f"    ‚ö†Ô∏è CUDA error during c2 generation, skipping")
                                    continue
                                else:
                                    raise
                        
                        for ref_output in ref_outputs:
                            ref_completion = tokenizer.decode(ref_output[ref_input_ids.shape[1]:], skip_special_tokens=True)
                            ref_code = extract_code_from_completion(ref_completion)
                            
                            print(f"    Generated c2: {ref_code[:60]}...")
                            
                            ref_exec_result = execute_code_with_tests(ref_code, problem["test_list"], problem.get("test_imports", []))
                            ref_reward = calculate_code_reward(ref_exec_result)
                            
                            all_rewards.append(ref_reward)
                            all_sequences.append(ref_output)
                            refinement_count += 1
                            
                            if ref_reward >= 1.0:
                                print(f"    üéØ c2 success! Reward: {ref_reward:.2f}")
                            else:
                                print(f"    üîÑ c2 failed. Reward: {ref_reward:.2f}")
            
            if not all_rewards:
                print(f"  No valid sequences, skipping training")
                continue
                
            avg_reward = sum(all_rewards) / len(all_rewards)
            success_rate = sum(1 for r in all_rewards if r >= 1.0) / len(all_rewards)
            print(f"  Avg reward: {avg_reward:.3f}, Success: {success_rate:.1%}")
            print(f"  Refinements: {refinement_count}")
            
            if avg_reward > 0 and len(all_sequences) >= 2:
                print(f"  üîÑ Skipping aggregated training; already trained per problem above")
             
            clear_memory()
        
        output_dir = "./models/srrl"
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print("‚úÖ SRRL completed!")
        
        # Cleanup model properly
        cleanup_model(model)
        return True
        
    except Exception as e:
        print(f"‚ùå SRRL failed: {e}")
        import traceback
        traceback.print_exc()
        if 'model' in locals():
            cleanup_model(model)
        return False

def evaluate_model(model_path, method_name, test_problems):
    print(f"\nüìä EVALUATING {method_name.upper()}")
    print("=" * 40)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from mbpp_utils import execute_code_with_tests, calculate_code_reward
        
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        # Use float32 during evaluation to avoid NaN/Inf in logits causing CUDA asserts
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,
            device_map="auto",
        )
        model.eval()
        device = next(model.parameters()).device
        
        results = []
        total_correct = 0
        total_reward = 0
        
        for i, problem in enumerate(test_problems):
            print(f"  Problem {i+1}/{len(test_problems)}: {problem['task_id']}")
            
            prompt = create_code_prompt(problem)
            
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=400)
            input_ids = inputs["input_ids"].to(device)
            attention_mask = inputs["attention_mask"].to(device)
            
            with torch.no_grad():
                try:
                    outputs = model.generate(
                        input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=1024,  # shorter generation to avoid overflow
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        pad_token_id=tokenizer.pad_token_id,
                        use_cache=True
                    )
                except RuntimeError as e:
                    if "CUDA" in str(e):
                        print(f"    ‚ö†Ô∏è CUDA error, resetting device and marking as failed")
                        try:
                            torch.cuda.empty_cache()
                            torch.cuda.device_reset()
                        except Exception:
                            pass
                        code = "# Generation failed"
                        exec_result = {"success": False, "error": "CUDA error", "failed_tests": [], "passed_tests": []}
                        reward = 0.0
                    else:
                        raise
                else:
                    completion = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
                    code = extract_code_from_completion(completion)
                    exec_result = execute_code_with_tests(code, problem["test_list"], problem.get("test_imports", []))
                    reward = calculate_code_reward(exec_result)
            
            success = reward >= 1.0
            
            result = {
                "task_id": problem["task_id"],
                "generated_code": code,
                "execution_success": exec_result["success"],
                "reward": reward,
                "success": success,
                "error": exec_result.get("error", ""),
                "failed_tests": exec_result.get("failed_tests", [])
            }
            results.append(result)
            
            total_reward += reward
            if success:
                total_correct += 1
                print(f"    ‚úÖ SUCCESS")
            else:
                print(f"    ‚ùå FAILED")
        
        accuracy = total_correct / len(test_problems)
        avg_reward = total_reward / len(test_problems)
        
        print(f"  Accuracy: {accuracy:.1%} ({total_correct}/{len(test_problems)})")
        print(f"  Avg Reward: {avg_reward:.3f}")
        
        return {
            "method": method_name.upper(),
            "accuracy": accuracy,
            "avg_reward": avg_reward,
            "correct_problems": total_correct,
            "total_problems": len(test_problems),
            "detailed_results": results
        }
        
    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return {"method": method_name.upper(), "error": str(e)}

def main():
    print("üéØ STABLE GRPO vs SRRL EXPERIMENT")
    print("=" * 50)
    
    from mbpp_utils import load_mbpp_dataset
    dataset = load_mbpp_dataset()
    
    train_problems = dataset[:3]
    test_problems = dataset[10:13]
    
    print(f"Training: {len(train_problems)} problems")
    print(f"Testing: {len(test_problems)} problems")
    print("üîß Fixed CUDA errors + XML prompts!")
    
    grpo_success = train_grpo(train_problems)
    
    # Force CUDA cleanup and wait between training phases to prevent corruption
    clear_memory()
    import time
    time.sleep(5)
    print("\n‚ö° CUDA cleanup complete, starting SRRL...")
    
    srrl_success = train_srrl(train_problems)
    
    if not (grpo_success or srrl_success):
        print("‚ùå Both failed!")
        return
    
    results = {
        "experiment_info": {
            "date": datetime.now().isoformat(),
            "model": "Qwen/Qwen2.5-3B-Instruct",
            "dataset": "MBPP",
            "training_problems": len(train_problems),
            "test_problems": len(test_problems),
            "improvements": "Numerical stability + proper prompts"
        }
    }
    
    # Force cleanup before evaluation
    clear_memory()
    time.sleep(2)
    print("\nüîç Starting evaluation...")
    
    if grpo_success:
        results["grpo"] = evaluate_model("./models/grpo", "grpo", test_problems)
        clear_memory()  # Clean up between model evaluations
    
    if srrl_success:
        results["srrl"] = evaluate_model("./models/srrl", "srrl", test_problems)
        clear_memory()
    
    if "grpo" in results and "srrl" in results and "error" not in results["grpo"] and "error" not in results["srrl"]:
        grpo_acc = results["grpo"]["accuracy"]
        srrl_acc = results["srrl"]["accuracy"]
        improvement = ((srrl_acc - grpo_acc) / grpo_acc * 100) if grpo_acc > 0 else 0
        
        results["comparison"] = {
            "grpo_accuracy": grpo_acc,
            "srrl_accuracy": srrl_acc,
            "accuracy_improvement": improvement,
            "winner": "SRRL" if srrl_acc > grpo_acc else "GRPO"
        }
        
        print(f"\nüèÜ FINAL RESULTS:")
        print(f"  GRPO: {grpo_acc:.1%}")
        print(f"  SRRL: {srrl_acc:.1%}")
        print(f"  Winner: {results['comparison']['winner']}")
    
    with open("experiment_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\n‚úÖ STABLE EXPERIMENT COMPLETE!")
    print("üìã Real results with CUDA fixes!")

if __name__ == "__main__":
    main()
